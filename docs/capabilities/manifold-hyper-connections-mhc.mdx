# mHC 範式：具身智能與大規模模型中的信號守恆

> 核心使命：解析 DeepSeek Manifold Hyper-Connections (mHC) 的底層邏輯，探討如何在架構層面強制執行「守恆定律」。

---

## 🧠 核心洞见

> **“約束不是限制，而是保證。”**

傳統 Transformer 的殘差路徑是 `x + F(x)`。超連接（Hyper-Connections）試圖將其擴張為多條並行流，但這帶來了「信號爆炸」的熵增風險。mHC 通過流形約束，將動態系統鎖定在穩定區間。

---

## 1. 為什麼會爆炸？（The Explosion Bug）

在標準殘差中，信息流是單一的。在超連接中，信息在 $n$ 條流中混合：
$x_{i+1} = \sum H \cdot x_i$

如果混合矩陣 $H$ 的行和或列和不對稱，信號就會在每一層被微小放大。在深層網絡中，這種放大呈指數級複合：
- **10M 參數**：放大約 9.2 倍（尚可存活）。
- **1.7B 參數**：放大 **10,924 倍**（極其危險）。
- **27B 參數**：潛在放大 **300,000+ 倍**（必然崩潰）。

---

## 2. Sinkhorn 投影：強制守恆

DeepSeek 的解決方案是利用 **Sinkhorn-Knopp 算法**。

### 2.1 算法邏輯
1.  **取指數**：使所有條目為正數（$e^H$）。
2.  **行歸一化**：使每行之和為 1。
3.  **列歸一化**：使每列之和為 1。
4.  **重複迭代**：直到收斂為雙擬隨機矩陣。

### 2.2 物理意義
雙擬隨機矩陣意味著混合操作只能是「加權平均」。它可以路由、混洗、融合信息，但 **絕對無法放大信號**。這將 Amax 鎖定在 1.0，實現了數值意義上的「能量守恆」。

---

## 3. 實踐中的「金絲雀」：第 0 層 (Layer 0)

複現實驗發現，不穩定性往往始於第 0 層。
- **原因**：這是唯一直接接觸原始數據（Embeddings）的層。
- **管理啟發**：在任何 Agent 系統或神經網絡中，**「輸入層的輸入質量與尺度」** 是穩定性的第一道防線。

---

## 4. 哲學思考：無聊的選擇與穩定性稅

- **性能稅**：在小規模下，mHC 的約束可能被視為對表達能力的「徵稅」（HC 表現稍好）。
- **穩定性紅利**：在大規模下，這份「稅收」是防止模型爆炸成 NaN 的唯一保險。
- **結論**：不要學習好的行為，要讓壞的行為變得不可能。

---

## 🔗 关联章节
- **[機器人引導程序理論](./robotics-bootstrap-theory.mdx)**
- **[瓶頸數據選取範式](./bottleneck-data-selection-paradigm.mdx)**
- **[04 Playbook：风险治理与回滚](../agent-management/04-playbook-risk-and-rollback.mdx)**
