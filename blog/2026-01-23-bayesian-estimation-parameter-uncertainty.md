# è¬›é€æœ€å¼·åƒæ•¸ä¼°è¨ˆï¼šè²è‘‰æ–¯ä¼°è¨ˆï¼ˆBayesian Estimationï¼‰æ·±åº¦è§£æ

> **ä¿¡è™Ÿä¾†æº**ï¼šcoså¤§å£¯ - åƒæ•¸ä¼°è¨ˆç³»åˆ—åˆ†äº«
> **æ ¸å¿ƒé—œéµè©**ï¼šä¸ç¢ºå®šæ€§å‚³æ’­ã€å…±è»›å…ˆé©—ã€å±¤ç´šæ¨¡å‹ã€æ¨¡å‹é¸æ“‡ï¼ˆå¥§å¡å§†å‰ƒåˆ€ï¼‰

---

å“ˆå˜ï¼Œæˆ‘æ˜¯ cos å¤§å£¯~ ä»Šå…’å’Œå¤§å®¶åˆ†äº«ä¸€å€‹åœ¨åƒæ•¸ä¼°è¨ˆæ–¹æ³•æ¶‰åŠåˆ°çš„ï¼š**è²è‘‰æ–¯ä¼°è¨ˆ**ã€‚

åœ¨åƒæ•¸ä¼°è¨ˆä¸­ï¼Œå‚³çµ±çš„é »ç‡å­¸æ´¾æ–¹æ³•ä¸»è¦ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆï¼ˆMLEï¼‰æˆ–æ­£å‰‡åŒ–çš„æœ€å¤§å¾Œé©—ä¼°è¨ˆï¼ˆMAPï¼‰ã€‚èˆ‡ä¹‹ä¸åŒï¼Œ**è²è‘‰æ–¯ä¼°è¨ˆå°‡åƒæ•¸è¦–ç‚ºéš¨æ©Ÿè®Šé‡**ï¼Œä½¿ç”¨å…ˆé©—åˆ†å¸ƒè¡¨é”æˆ‘å€‘çš„å…ˆé©—çŸ¥è­˜ï¼Œé€šéæ•¸æ“šçš„ä¼¼ç„¶å‡½æ•¸æ›´æ–°åˆ°å¾Œé©—åˆ†å¸ƒï¼Œä¸¦æœ€çµ‚åŸºæ–¼å¾Œé©—é€²è¡Œæ¨æ–·èˆ‡é æ¸¬ã€‚

## ğŸ§  è²è‘‰æ–¯ä¼°è¨ˆçš„æ ¸å¿ƒï¼šBayes å®šç†

\[ P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)} \]

å…¶ä¸­ï¼š
- **\( P(\theta) \)**ï¼šå…ˆé©—ï¼ˆPriorï¼‰ã€‚
- **\( P(D | \theta) \)**ï¼šä¼¼ç„¶ï¼ˆLikelihoodï¼‰ã€‚
- **\( P(\theta | D) \)**ï¼šå¾Œé©—ï¼ˆPosteriorï¼‰ã€‚
- **\( P(D) \)**ï¼šè­‰æ“šï¼ˆEvidenceï¼‰ï¼Œç”¨æ–¼æ¨¡å‹æ¯”è¼ƒã€‚

èˆ‡ MLE/MAP ä¸åŒçš„æ˜¯ï¼Œè²è‘‰æ–¯ä¼°è¨ˆé€šéå¾Œé©—åˆ†å¸ƒåæ˜ åƒæ•¸ä¸ç¢ºå®šæ€§ï¼Œé æ¸¬æ™‚é€²ä¸€æ­¥å°‡åƒæ•¸ä¸ç¢ºå®šæ€§å‚³æ’­åˆ°è¼¸å‡ºï¼Œå¾—åˆ° **å¾Œé©—é æ¸¬åˆ†å¸ƒ**ã€‚é€™ä½¿æˆ‘å€‘èƒ½å¤ åš´æ ¼åœ°çµ¦å‡ºç½®ä¿¡å€é–“ï¼Œä¸¦é€²è¡Œç©©å¥çš„æ¨¡å‹æ¯”è¼ƒã€‚

---

## ğŸ› ï¸ æ¡ˆä¾‹æ¨¡å‹ï¼šè²è‘‰æ–¯ç·šæ€§å›æ­¸

æˆ‘å€‘è€ƒæ…®ä¸€ç¶­è¼¸å…¥ \( x \)ã€æ¨™é‡è¼¸å‡º \( y \)ï¼Œæ¡ç”¨éç·šæ€§ç‰¹å¾µæ˜ å°„ï¼ˆä¾‹å¦‚å¤šé …å¼åŸºï¼‰ã€‚

### 1. ä¼¼ç„¶å‡½æ•¸ (Likelihood)
åœ¨çµ¦å®š \(\mathbf{w}\) å’Œ \(\sigma^2\) æƒ…æ³ä¸‹ï¼Œä¼¼ç„¶ç‚ºé«˜æ–¯ï¼š
\[ P(\mathbf{y} | \mathbf{X}, \mathbf{w}, \beta) = \mathcal{N}(\mathbf{y} | \mathbf{\Phi}\mathbf{w}, \beta^{-1}\mathbf{I}) \]
å…¶ä¸­ \(\beta = 1/\sigma^2\) æ˜¯å™ªè²ç²¾åº¦ã€‚

### 2. å…ˆé©—é¸æ“‡ (Prior)
- **æ¬Šé‡å…ˆé©—**ï¼š\( P(\mathbf{w} | \alpha) = \mathcal{N}(\mathbf{w} | \mathbf{0}, \alpha^{-1}\mathbf{I}) \)ã€‚
- **å±¤ç´šå…ˆé©— (NIG)**ï¼šå° \(\sigma^2\) ä½¿ç”¨ Inverse-Gamma åˆ†å¸ƒï¼Œå½¢æˆå±¤ç´šè²è‘‰æ–¯æ¨¡å‹ï¼Œåœ¨å°æ¨£æœ¬ä¸‹æ›´ç©©å¥ã€‚

### 3. å¾Œé©—é æ¸¬åˆ†å¸ƒ (Posterior Predictive)
å°æ–¼æ–°è¼¸å…¥ \(\mathbf{x}_*\)ï¼Œé æ¸¬åˆ†å¸ƒä»ç‚ºé«˜æ–¯ï¼š
\[ P(y_* | \mathbf{x}_*, D) = \mathcal{N}(y_* | \mu_* , \sigma^2_*) \]
å…¶ä¸­é æ¸¬æ–¹å·® \(\sigma^2_*\) ç”± **å™ªè²** èˆ‡ **åƒæ•¸ä¸ç¢ºå®šæ€§** å…±åŒè²¢ç»ã€‚

---

## ğŸ’» å®Œæ•´ä»£ç¢¼å¯¦ç¾ (PyTorch & Sklearn)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import BayesianRidge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
import torch
from torch.distributions import MultivariateNormal

np.random.seed(42)
torch.manual_seed(42)

# 1) åˆæˆæ•¸æ“š
N = 80
X = np.linspace(-3, 3, N)
def f_true(x):
    return np.sin(1.5*x) + 0.3*(x**2) - 0.5*x

sigma = 0.5 
y = f_true(X) + np.random.normal(0, sigma, size=N)

# 2) æ§‹é€ å¤šé …å¼ç‰¹å¾µ
M = 7  
poly = PolynomialFeatures(degree=M, include_bias=True)
Phi = poly.fit_transform(X.reshape(-1, 1))  
M_eff = Phi.shape[1]  

# 3) PyTorch è¨ˆç®—å¾Œé©— (å…±è»›å…¬å¼)
alpha = 1.0   
beta = 1.0 / (sigma**2)  

Phi_t = torch.tensor(Phi, dtype=torch.float64)
y_t = torch.tensor(y, dtype=torch.float64).reshape(-1, 1)

A = alpha * torch.eye(M_eff, dtype=torch.float64) + beta * (Phi_t.T @ Phi_t)
Sigma = torch.linalg.inv(A)
mu = beta * (Sigma @ (Phi_t.T @ y_t))  

# å¾Œé©—é æ¸¬å‡½æ•¸
def posterior_predict(x_grid, mu, Sigma, poly, beta):
    Phi_star = poly.transform(x_grid.reshape(-1, 1))
    Phi_star_t = torch.tensor(Phi_star, dtype=torch.float64)
    mean = (Phi_star_t @ mu).numpy().flatten()
    var = (1.0/beta) + torch.sum(Phi_star_t @ Sigma * Phi_star_t, dim=1)
    return mean, var.numpy()

# 4) å¯è¦–åŒ–
X_test = np.linspace(-3.5, 3.5, 400)
mean_post, var_post = posterior_predict(X_test, mu, Sigma, poly, beta)
std_post = np.sqrt(var_post)

plt.figure(figsize=(12, 8))
plt.scatter(X, y, color='#e41a1c', s=35, alpha=0.8, label='è§€æ¸¬æ•¸æ“š')
plt.plot(X_test, f_true(X_test), '--', color='black', label='çœŸå¯¦å‡½æ•¸')
plt.plot(X_test, mean_post, color='#377eb8', lw=2.5, label='å¾Œé©—é æ¸¬å‡å€¼')
plt.fill_between(X_test, mean_post - 1.96*std_post, mean_post + 1.96*std_post, color='#4daf4a', alpha=0.2, label='95%å¯ä¿¡å€é–“')
plt.title('è²è‘‰æ–¯ç·šæ€§å›æ­¸ï¼šä¸ç¢ºå®šæ€§å‚³æ’­')
plt.legend()
plt.show()

# æ‰“å° sklearn ä¼°è¨ˆçµæœ
br_model = Pipeline([("poly", poly), ("bayes", BayesianRidge())])
br_model.fit(X.reshape(-1, 1), y)
print(f"Sklearn Alpha (æ¬Šé‡ç²¾åº¦): {br_model.named_steps['bayes'].alpha_}")
```

---

## ğŸ“Š æ ¸å¿ƒå•Ÿç¤ºï¼šç‚ºä»€éº¼è²è‘‰æ–¯æ›´å¼·ï¼Ÿ

1.  **ä¸ç¢ºå®šæ€§é‡åŒ–**ï¼šå®ƒä¸åªçµ¦ä½ ä¸€å€‹ã€Œé»ã€ï¼Œé‚„çµ¦ä½ ä¸€å€‹ã€Œç¯„åœã€ã€‚é€™åœ¨ Agent æ±ºç­–ï¼ˆå¦‚è‡ªå‹•é§•é§›ã€é¢¨æ§ï¼‰ä¸­è‡³é—œé‡è¦ã€‚
2.  **è‡ªå‹•å¥§å¡å§†å‰ƒåˆ€**ï¼šé‚Šç·£ä¼¼ç„¶ï¼ˆEvidenceï¼‰æœƒè‡ªå‹•æ‡²ç½°éæ–¼è¤‡é›œçš„æ¨¡å‹ï¼Œå¤©ç„¶å¹³è¡¡æ“¬åˆåº¦èˆ‡è¤‡é›œåº¦ã€‚
3.  **åƒæ•¸ç›¸é—œæ€§**ï¼šå¾Œé©—å”æ–¹å·®çŸ©é™£æ­ç¤ºäº†åƒæ•¸é–“çš„å”ä½œé—œä¿‚ï¼Œå¹«ä½ è­˜åˆ¥å“ªäº›ç‰¹å¾µæ˜¯ã€Œå±éšªã€æˆ–ä¸ç©©å®šçš„ã€‚

è©³ç´°ç†è«–èˆ‡å±¤ç´šæ¨¡å‹æ¨å°ï¼Œè«‹åƒé–±ï¼š
- **[ä¸ç¢ºå®šæ€§è™•ç†èˆ‡è²è‘‰æ–¯ä¼°è¨ˆ](../../stack/frameworks/bayesian-estimation-and-uncertainty.mdx)**
