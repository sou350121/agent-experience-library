# 具身智能的範式轉移：從 Action 到 World Model

恭喜 1X 展示了具身智能的新路線。如果說之前的 VLA 是在訓練「反射神經」，那麼 1X World Model (1XWM) 就是在賦予機器人「想象力」。

### 1️⃣ 為什麼 VLA 不夠？
傳統的 VLA（Vision-Language-Action）通常是「看一幀 -> 出一個動作」。這就像是人在閉著眼睛打球，缺乏對物理動態的長時序預判。

### 2️⃣ 1X 的解法：想象成功，然後執行
1X 的核心思路非常硬核：
- **生成未來**：給定指令後，先生成一段「接下來會發生什麼」的視頻。
- **動作對齊**：用逆動力學模型把這段視頻「翻譯」成電機指令。

### 3️⃣ 數據套利：互聯網視頻是礦山
這套範式最強大的地方在於，它不需要昂貴的人工遙操作數據來學習物理規律。它直接在互聯網海量視頻中學習「杯子碎了會怎樣」、「門是怎麼推開的」。

### 4️⃣ 對 Agent 時代的啟示
未來的 Agent 不僅要會寫代碼、會聊天，還要具備「世界模型」。在數字世界中模擬，在物理世界中閉環。

---
更多細節見庫中文檔：[1X World Model 範式解析](../stack/frameworks/1x-world-model-paradigm.mdx)
