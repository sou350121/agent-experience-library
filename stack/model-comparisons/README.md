# 模型对比 (Model Comparisons)

> **核心使命：** 拒绝跑分，只看工程实战。基于真实编码场景、推理性能与交付质量，对主流大模型进行深度横向对比。

---

## 🧭 推荐阅读顺序

1. **[GPT-5 vs Claude-4.5：2026 旗舰之战](./gpt-5-vs-claude-4.5.mdx)**
   - 谁才是真正的“工程之王”？从多轮对话一致性到长上下文召回。
2. **[GPT-5 vs Opus-4.5：SSE 长序列流式工程案例对比](./gpt-5-vs-opus-4.5-sse-case.mdx)**
   - 硬核场景：在处理复杂的流式数据交互时，不同模型的逻辑稳定性。
3. **[GLM-4.7 实测：国产 Agent 首选模型的进化](./glm-4.7-review.mdx)**
   - 关注中文语境、内網工具調用與性價比的選型建議。

---

## ⭐ 测评维度索引

- **逻辑深度**：能否理解复杂的业务因果链？
- **工程遵从度**：是否会私自修改你的 `STEERING.md` 或忽略 `AGENT.md`？
- **代码审美**：生成的代码是“能跑就行”还是具备工业级健壮性？
- **工具调用 (MCP)**：在复杂工具链下的响应成功率与容错性。

---

## 🧠 核心洞见

1️⃣ **拒绝对跑分的崇拜，建立“工程信任感”**
- **逻辑思考**：跑分（Benchmarks）是实验室里的静态快照，而工程交付是动态的博弈。真正决定选型的不是模型能跑多少分，而是它对你的 `AGENT.md` 和 `STEERING.md` 的遵从度。
- **启发**：一个“听话”的 7B 模型（如 RLinf 优化版）在特定任务中可能比一个“自作聪明”的旗舰模型更高效。

2️⃣ **长上下文不是万能药，召回率才是**
- **逻辑思考**：能读 200k 窗口不代表能处理 200k 信息。在长文本中迷失（Lost in the Middle）是模型的通病。
- **启发**：与其指望模型一次读完整个 repo，不如通过 RAG 和 语义路由 精准喂食。

3️⃣ **模型即插件，接口即未来**
- **逻辑思考**：模型能力在快速迭代，今天的新王可能是明天的废纸。
- **启发**：保持架构的松耦合（如使用 MCP），让你的系统能够随模型迭代而无感切换。

---

## 🔗 关联章节
- **[能力边界](../capabilities/README.md)**：了解模型表现差异背后的理论根源。
- **[案例复盘](../case-studies/README.md)**：看这些模型在具体项目中究竟遇到了哪些挑战。
