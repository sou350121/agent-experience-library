# 锯齿状智能与 RLVR：理解 2025 推理模型

2025 年，AI 的演进不再单纯依赖参数规模的暴涨，而是转向了**可验证环境下的强化学习**。

## 1. RLVR：新一代 Scaling Law
**RLVR (Reinforcement Learning from Verifiable Rewards)** 是 2025 年的事实标准。

- **核心机制**：在数学、代码、逻辑等具备“客观对错”的环境中，让模型自发摸索推理策略。
- **范式转变**：
  - **SFT/RLHF**：人类教 AI “怎么想”（难以设计最优路径）。
  - **RLVR**：环境奖惩让 AI 自己寻找“解题轨迹”。
- **推理时缩放 (Test-time Scaling)**：通过生成更长的思考链（CoT），投入更多算力换取更高智能。

## 2. 锯齿状智能 (Jagged Intelligence)
Karpathy 认为我们召唤的是“幽灵”而非“动物”。

- **非均匀分布**：模型能力不再是平滑的曲线。在 RLVR 覆盖的“尖刺区域”（如代码、数学），表现如天才；在模糊区域，表现如小学生。
- **基准测试的失效**：因为 Benchmark 本身就是可验证环境，RLVR 极易在测试点附近培育“能力突起”，导致高分模型在现实任务中依然可能翻车。

## 3. 对 Agent 开发的指导意义
1. **优先处理可验证任务**：Agent 在能进行逻辑自洽检查的领域（如编译运行、单元测试、数学计算）上限极高。
2. **警惕“智力陷阱”**：不要因为 Agent 会写复杂的算法，就默认它懂基本的常识或隐私红线。
3. **拥抱“长思考”**：在复杂决策中，给推理模型（如 O1/GPT-5.2）足够的“思考步数”是换取成功率的关键。

---
> 💡 **总结**：2025 年的智能是高度不均衡的。理解“锯齿”的形状，避开低谷，利用巅峰，是架构师的必修课。

