# MediaPipe æ‰‹åŠ¿è¯†åˆ«å®Œæ•´æ•™ç¨‹

## ç¬¬ä¸€æ­¥ï¼šäº†è§£ MediaPipe Hands

**MediaPipe Hands** æ˜¯ Google å¼€å‘çš„é«˜æ€§èƒ½æ‰‹éƒ¨è¿½è¸ªè§£å†³æ–¹æ¡ˆï¼š

- âœ… **21 ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹**æ£€æµ‹
- âœ… **å®æ—¶è¿½è¸ª**ï¼Œé«˜ç²¾åº¦ä½å»¶è¿Ÿ
- âœ… **å•æ‰‹/åŒæ‰‹**æ£€æµ‹
- âœ… **è·¨å¹³å°**æ”¯æŒï¼ˆWindows/Linux/Mac/Android/iOSï¼‰
- âœ… **å…è´¹å¼€æº**ï¼ŒMIT è®¸å¯è¯
- âœ… **CPU è¿è¡Œ**ï¼Œæ— éœ€ GPU

## ç¬¬äºŒæ­¥ï¼šå®‰è£…å‡†å¤‡

### 1. ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- æ‘„åƒå¤´ï¼ˆå†…ç½®æˆ– USBï¼‰
- è‡³å°‘ 4GB RAM

### 2. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install opencv-python
pip install mediapipe
pip install numpy

# å®‰è£…é¢å¤–ä¾èµ–ï¼ˆæŸäº›åŠŸèƒ½éœ€è¦ï¼‰
pip install pyautogui  # é¼ æ ‡æ§åˆ¶
pip install cvzone  # ç®€åŒ–çš„ Computer Vision åº“
```

### 3. éªŒè¯å®‰è£…

```bash
python -c "import cv2, mediapipe; print('âœ… å®‰è£…æˆåŠŸ!')"
python -c "import mediapipe; print('MediaPipe ç‰ˆæœ¬:', mediapipe.__version__)"
```

## ç¬¬ä¸‰æ­¥ï¼šç†è§£ 21 ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹

MediaPipe æ£€æµ‹ 21 ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹ï¼š

```
æ‰‹è…•: 0
æ‹‡æŒ‡: [1, 2, 3, 4]     -> 4 æ˜¯æŒ‡å°–
é£ŸæŒ‡: [5, 6, 7, 8]     -> 8 æ˜¯æŒ‡å°–
ä¸­æŒ‡: [9, 10, 11, 12]  -> 12 æ˜¯æŒ‡å°–
æ— åæŒ‡: [13, 14, 15, 16] -> 16 æ˜¯æŒ‡å°–
å°æŒ‡: [17, 18, 19, 20] -> 20 æ˜¯æŒ‡å°–
```

**æœ€å¸¸ç”¨çš„å…³é”®ç‚¹ï¼š**
- **0**: æ‰‹è…•ï¼ˆåŸºå‡†ç‚¹ï¼‰
- **4, 8, 12, 16, 20**: 5 ä¸ªæ‰‹æŒ‡å°–
- **8**: é£ŸæŒ‡æŒ‡å°–ï¼ˆæœ€å¸¸ç”¨ï¼‰

## ç¬¬å››æ­¥ï¼šè¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€æ‰‹éƒ¨æ£€æµ‹

åˆ›å»º `example1_basic_detection.py`ï¼š

```python
import cv2
import mediapipe as mp

def main():
    print("ğŸ¯ MediaPipe æ‰‹éƒ¨æ£€æµ‹ç¤ºä¾‹")
    print("=" * 50)

    # åˆå§‹åŒ– MediaPipe Hands
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        static_image_mode=False,    # è§†é¢‘æµæ¨¡å¼
        max_num_hands=2,            # æœ€å¤šæ£€æµ‹2åªæ‰‹
        min_detection_confidence=0.7,  # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        min_tracking_confidence=0.5    # è¿½è¸ªç½®ä¿¡åº¦é˜ˆå€¼
    )

    # åˆå§‹åŒ–ç»˜å›¾å·¥å…·
    mp_draw = mp.solutions.drawing_utils
    mp_draw_styles = mp.solutions.drawing_styles

    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)

    if not cap.isOpened():
        print("âŒ é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´!")
        return

    print("âœ… æ‘„åƒå¤´å·²å¯åŠ¨! å¼€å§‹æ£€æµ‹æ‰‹éƒ¨...")
    print("æç¤º: æŒ‰ 'q' é€€å‡º")

    while True:
        success, img = cap.read()
        if not success:
            break

        # è½¬æ¢é¢œè‰²ç©ºé—´ BGR -> RGB
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # å¤„ç†å›¾åƒ
        results = hands.process(img_rgb)

        # å¦‚æœæ£€æµ‹åˆ°æ‰‹éƒ¨
        if results.multi_hand_landmarks:
            for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
                print(f"âœ… æ£€æµ‹åˆ°ç¬¬ {hand_idx + 1} åªæ‰‹")

                # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹å’Œè¿æ¥çº¿
                mp_draw.draw_landmarks(
                    img,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_draw_styles.get_default_hand_landmarks_style(),
                    mp_draw_styles.get_default_hand_connections_style()
                )

                # è·å–é£ŸæŒ‡æŒ‡å°–åæ ‡
                index_finger_tip = hand_landmarks.landmark[8]
                h, w, c = img.shape
                cx, cy = int(index_finger_tip.x * w), int(index_finger_tip.y * h)
                print(f"   é£ŸæŒ‡æŒ‡å°–: X={cx}, Y={cy}")

                # åœ¨é£ŸæŒ‡å°–ä½ç½®ç”»åœ†
                cv2.circle(img, (cx, cy), 10, (255, 0, 0), cv2.FILLED)

        # æ˜¾ç¤ºå›¾åƒ
        cv2.imshow("Hand Tracking - Press Q to quit", img)

        # æŒ‰ 'q' é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š
```bash
python example1_basic_detection.py
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… æ‘„åƒå¤´ç”»é¢æ˜¾ç¤º
- âœ… æ‰‹éƒ¨éª¨æ¶å’Œå…³é”®ç‚¹å¯è§†åŒ–
- âœ… 21 ä¸ªå…³é”®ç‚¹ç”¨ä¸åŒé¢œè‰²æ ‡è®°
- âœ… é£ŸæŒ‡æŒ‡å°–æœ‰è“è‰²åœ†ç‚¹æ ‡è®°

## ç¬¬äº”æ­¥ï¼šæ‰‹åŠ¿è¯†åˆ«åŸç†

### æ‰‹æŒ‡çŠ¶æ€åˆ¤æ–­

åˆ¤æ–­æ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´ï¼š

```python
def finger_is_up(landmarks, finger_tip_idx, finger_pip_idx):
    """åˆ¤æ–­æ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´"""
    return landmarks.landmark[finger_tip_idx].y < landmarks.landmark[finger_pip_idx].y

# åˆ¤æ–­æ¯ä¸ªæ‰‹æŒ‡
thumb_up = finger_is_up(hand_landmarks, 4, 2)
index_up = finger_is_up(hand_landmarks, 8, 6)
middle_up = finger_is_up(hand_landmarks, 12, 10)
ring_up = finger_is_up(hand_landmarks, 16, 14)
pinky_up = finger_is_up(hand_landmarks, 20, 18)

fingers_up = [thumb_up, index_up, middle_up, ring_up, pinky_up]
```

### å¸¸è§æ‰‹åŠ¿è¯†åˆ«

#### 1. æ¡æ‹³ï¼ˆFistï¼‰
æ‰€æœ‰æ‰‹æŒ‡å¼¯æ›²
```python
if sum(fingers_up) == 0:
    gesture = "âœŠ æ¡æ‹³"
```

#### 2. å¼ å¼€æ‰‹æŒï¼ˆOpen Palmï¼‰
æ‰€æœ‰æ‰‹æŒ‡ä¼¸ç›´
```python
if sum(fingers_up) == 5:
    gesture = "âœ‹ å¼ å¼€æ‰‹æŒ"
```

#### 3. ç‚¹èµï¼ˆThumbs Upï¼‰
åªæœ‰æ‹‡æŒ‡ä¼¸ç›´
```python
if fingers_up == [True, False, False, False, False]:
    gesture = "ğŸ‘ ç‚¹èµ"
```

#### 4. Vå­—æ‰‹åŠ¿ï¼ˆVictoryï¼‰
åªæœ‰é£ŸæŒ‡å’Œä¸­æŒ‡ä¼¸ç›´
```python
if fingers_up == [False, True, True, False, False]:
    gesture = "âœŒï¸ Vå­—æ‰‹åŠ¿"
```

#### 5. æåˆï¼ˆPinchï¼‰
æ‹‡æŒ‡å’Œé£ŸæŒ‡æŒ‡å°–è·ç¦»å¾ˆè¿‘
```python
thumb_tip = hand_landmarks.landmark[4]
index_tip = hand_landmarks.landmark[8]

distance = ((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)**0.5

if distance < 0.05:  # é˜ˆå€¼å¯è°ƒ
    gesture = "ğŸ‘Œ æåˆ"
```

## ç¬¬å…­æ­¥ï¼šé«˜çº§åŠŸèƒ½

### 1. å¹³æ»‘è¿½è¸ªï¼ˆå‡å°‘æŠ–åŠ¨ï¼‰

```python
from collections import deque

class SmoothTracker:
    def __init__(self, window_size=5):
        self.buffer = deque(maxlen=window_size)

    def smooth(self, x, y):
        self.buffer.append((x, y))
        avg_x = sum(p[0] for p in self.buffer) / len(self.buffer)
        avg_y = sum(p[1] for p in self.buffer) / len(self.buffer)
        return int(avg_x), int(avg_y)

tracker = SmoothTracker(window_size=5)
```

### 2. æ‰‹åŠ¿å†å²è®°å½•

```python
from collections import deque

gesture_history = deque(maxlen=10)

def process_gesture(gesture):
    gesture_history.append(gesture)

    # æ£€æµ‹ç‰¹å®šåºåˆ—
    if list(gesture_history) == ["open_palm", "fist", "open_palm"]:
        return "zoom_in"

    return gesture
```

### 3. å¤šæ‰‹æ£€æµ‹

```python
if results.multi_hand_landmarks:
    for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
        # è·å–å·¦å³æ‰‹ä¿¡æ¯
        handedness = results.multi_handedness[idx].classification[0].label
        print(f"ç¬¬ {idx + 1} åªæ‰‹: {handedness}")
```

### 4. åæ ‡è½¬æ¢

```python
# å›¾åƒåæ ‡è½¬æ¢ä¸ºå±å¹•åæ ‡
def image_to_screen(img_x, img_y, img_w, img_h, screen_w, screen_h):
    screen_x = int(img_x / img_w * screen_w)
    screen_y = int(img_y / img_h * screen_h)
    return screen_x, screen_y
```

## ç¬¬ä¸ƒæ­¥ï¼šå®æˆ˜é¡¹ç›®

### é¡¹ç›® 1ï¼šæ‰‹åŠ¿è®¡æ•°å™¨

```python
import cv2
import mediapipe as mp
import time

def count_fingers(landmarks):
    """è®¡ç®—ä¼¸ç›´çš„æ‰‹æŒ‡æ•°é‡"""
    finger_tips = [8, 12, 16, 20]
    finger_pips = [6, 10, 14, 18]

    fingers_up = 0

    # æ‹‡æŒ‡ç‰¹æ®Šå¤„ç†ï¼ˆæ¯”è¾ƒ x åæ ‡ï¼‰
    if landmarks.landmark[4].x < landmarks.landmark[3].x:
        fingers_up += 1

    # å…¶ä»–å››ä¸ªæ‰‹æŒ‡
    for tip, pip in zip(finger_tips, finger_pips):
        if landmarks.landmark[tip].y < landmarks.landmark[pip].y:
            fingers_up += 1

    return fingers_up

def main():
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(max_num_hands=2)
    mp_draw = mp.solutions.drawing_utils

    cap = cv2.VideoCapture(0)

    while True:
        success, img = cap.read()
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # è®¡ç®—ä¼¸ç›´çš„æ‰‹æŒ‡æ•°é‡
                count = count_fingers(hand_landmarks)

                # æ˜¾ç¤ºæ•°å­—
                cv2.putText(img, str(count), (100, 150),
                          cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 0), 5)

        cv2.imshow("Finger Counter", img)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

### é¡¹ç›® 2ï¼šæ‰‹åŠ¿ç»˜å›¾æ¿

```python
import cv2
import mediapipe as mp
import numpy as np

class DrawingBoard:
    def __init__(self):
        self.canvas = None
        self.prev_pos = None
        self.drawing = False
        self.color = (255, 0, 0)
        self.brush_size = 5

    def draw(self, img, hand_landmarks):
        if self.canvas is None:
            self.canvas = np.zeros_like(img)

        h, w, _ = img.shape
        index_tip = hand_landmarks.landmark[8]
        thumb_tip = hand_landmarks.landmark[4]

        x, y = int(index_tip.x * w), int(index_tip.y * h)

        # æ£€æŸ¥æåˆ
        distance = ((index_tip.x - thumb_tip.x)**2 +
                   (index_tip.y - thumb_tip.y)**2)**0.5

        if distance < 0.05:  # æåˆçŠ¶æ€
            self.drawing = True
            if self.prev_pos:
                cv2.line(self.canvas, self.prev_pos, (x, y),
                        self.color, self.brush_size)
            self.prev_pos = (x, y)
        else:
            self.drawing = False
            self.prev_pos = None

        # åˆå¹¶å›¾åƒ
        img = cv2.addWeighted(img, 0.7, self.canvas, 0.3, 0)

        # æ˜¾ç¤ºæ¨¡å¼
        mode = "Drawing" if self.drawing else "Moving"
        cv2.putText(img, f"Mode: {mode}", (10, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        return img

def main():
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(max_num_hands=1)
    mp_draw = mp.solutions.drawing_utils

    cap = cv2.VideoCapture(0)
    board = DrawingBoard()

    while True:
        success, img = cap.read()
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                img = board.draw(img, hand_landmarks)

        cv2.imshow("Gesture Drawing", img)

        key = cv2.waitKey(1)
        if key & 0xFF == ord('q'):
            break
        elif key & 0xFF == ord('c'):  # æ¸…é™¤ç”»å¸ƒ
            board.canvas = np.zeros_like(img)

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. é™ä½åˆ†è¾¨ç‡

```python
# è®¾ç½®è¾ƒä½åˆ†è¾¨ç‡ä»¥æé«˜æ€§èƒ½
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
```

### 2. è°ƒæ•´æ£€æµ‹é¢‘ç‡

```python
frame_count = 0
detection_every = 3  # æ¯3å¸§æ£€æµ‹ä¸€æ¬¡

while True:
    success, img = cap.read()

    if frame_count % detection_every == 0:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)

    frame_count += 1
```

### 3. å‡å°‘æ£€æµ‹çš„æ‰‹æ•°

```python
hands = mp_hands.Hands(
    max_num_hands=1,  # åªæ£€æµ‹1åªæ‰‹
    min_detection_confidence=0.5,  # é™ä½é˜ˆå€¼
    min_tracking_confidence=0.5
)
```

## å¸¸è§é—®é¢˜è§£å†³

### Q1: æ£€æµ‹ä¸å‡†ç¡®
```python
# è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼
hands = mp_hands.Hands(
    min_detection_confidence=0.5,  # é™ä½é˜ˆå€¼
    min_tracking_confidence=0.5
)
```

### Q2: å»¶è¿Ÿå¤ªé«˜
```python
# é™ä½åˆ†è¾¨ç‡
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# å‡å°‘æ£€æµ‹çš„æ‰‹æ•°
hands = mp_hands.Hands(max_num_hands=1)
```

### Q3: æ‰‹æŒ‡æ–¹å‘åˆ¤æ–­é”™è¯¯
```python
# å¯¹äºæ‹‡æŒ‡ï¼Œéœ€è¦è€ƒè™‘æ‰‹çš„å·¦å³æ–¹å‘
handedness = results.multi_handedness[0].classification[0].label

if handedness == "Left":
    # å·¦æ‰‹çš„æ‹‡æŒ‡åˆ¤æ–­é€»è¾‘
    thumb_up = landmarks.landmark[4].x < landmarks.landmark[3].x
else:
    # å³æ‰‹çš„æ‹‡æŒ‡åˆ¤æ–­é€»è¾‘
    thumb_up = landmarks.landmark[4].x > landmarks.landmark[3].x
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 

1. ğŸ¯ å®ç°æ›´å¤æ‚çš„æ‰‹åŠ¿ç»„åˆ
2. ğŸ¨ åˆ›å»ºè‡ªå·±çš„æ‰‹åŠ¿åº”ç”¨
3. âš¡ ä¼˜åŒ–æ€§èƒ½å’Œå‡†ç¡®æ€§
4. ğŸŒ é›†æˆåˆ°å®é™…é¡¹ç›®ä¸­
5. ğŸ¤– ç»“åˆæœºå™¨å­¦ä¹ è¿›è¡Œè‡ªå®šä¹‰æ‰‹åŠ¿è¯†åˆ«

## å‚è€ƒèµ„æº

- [MediaPipe å®˜æ–¹æ–‡æ¡£](https://google.github.io/mediapipe/)
- [MediaPipe Hands è§£å†³æ–¹æ¡ˆ](https://google.github.io/mediapipe/solutions/hands.html)
- [OpenCV å®˜æ–¹æ–‡æ¡£](https://docs.opencv.org/4.x/)
- [GitHub ç¤ºä¾‹ä»£ç ](https://github.com/google/mediapipe/tree/master/examples/python)

---

**æ­å–œï¼æ‚¨å·²ç»æŒæ¡äº† MediaPipe æ‰‹åŠ¿è¯†åˆ«çš„åŸºç¡€ï¼** ğŸ‰

ç°åœ¨å¯ä»¥å¼€å§‹åˆ›å»ºè‡ªå·±çš„æ‰‹åŠ¿æ§åˆ¶åº”ç”¨äº†ï¼
