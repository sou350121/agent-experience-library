# Wujihandpy å®Œæ•´å…¥é—¨æ•™ç¨‹

## ç¬¬ä¸€æ­¥ï¼šäº†è§£é¡¹ç›®

**Wujihandpy** æ˜¯æ— å°½ç§‘æŠ€ï¼ˆWuji Technologyï¼‰å¼€å‘çš„ä¸€ä¸ª Python æ‰‹åŠ¿è¯†åˆ«å’Œæ§åˆ¶åº“ï¼Œä¸»è¦åŠŸèƒ½ï¼š
- ğŸ–ï¸ å®æ—¶æ‰‹éƒ¨æ£€æµ‹å’Œè¿½è¸ª
- ğŸ‘† æ‰‹åŠ¿è¯†åˆ«ï¼ˆæåˆã€ç‚¹å‡»ã€æ‹–æ‹½ç­‰ï¼‰
- ğŸ–±ï¸ é€šè¿‡æ‰‹åŠ¿æ§åˆ¶è®¡ç®—æœºé¼ æ ‡å’Œé”®ç›˜
- ğŸ“¹ æ”¯æŒæ‘„åƒå¤´è¾“å…¥
- ğŸ¯ é«˜ç²¾åº¦ä½å»¶è¿Ÿ

## ç¬¬äºŒæ­¥ï¼šå®‰è£…å‡†å¤‡

### 1. ç¡¬ä»¶è¦æ±‚
- âœ… æ‘„åƒå¤´ï¼ˆå†…ç½®æˆ–å¤–æ¥ USB æ‘„åƒå¤´ï¼‰
- âœ… è‡³å°‘ 4GB RAM
- âœ… æ”¯æŒ Python 3.8+ çš„ç³»ç»Ÿ

### 2. è½¯ä»¶è¦æ±‚
- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- pip åŒ…ç®¡ç†å™¨

### 3. å®‰è£… wujihandpy

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/wuji-technology/wujihandpy.git
cd wujihandpy

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£… wujihandpy
pip install -e .
```

æˆ–è€…ç›´æ¥ä» PyPI å®‰è£…ï¼ˆå¦‚æœå·²å‘å¸ƒï¼‰ï¼š
```bash
pip install wujihandpy
```

### 4. éªŒè¯å®‰è£…

```bash
python -c "import wujihandpy; print('Wujihandpy å®‰è£…æˆåŠŸ!')"
```

## ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

### åŸºç¡€æ‰‹åŠ¿æ£€æµ‹ç¤ºä¾‹

åˆ›å»ºæ–‡ä»¶ `my_first_gesture.py`ï¼š

```python
from wujihandpy import HandDetector
import cv2

def main():
    # åˆå§‹åŒ–æ‰‹éƒ¨æ£€æµ‹å™¨
    detector = HandDetector(
        max_hands=2,              # æœ€å¤šæ£€æµ‹2åªæ‰‹
        detection_con=0.7,        # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        tracking_con=0.5          # è¿½è¸ªç½®ä¿¡åº¦é˜ˆå€¼
    )

    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)

    while True:
        success, img = cap.read()
        if not success:
            break

        # æ£€æµ‹æ‰‹éƒ¨
        img = detector.find_hands(img)
        lm_list = detector.find_position(img)

        # å¦‚æœæ£€æµ‹åˆ°æ‰‹éƒ¨å…³é”®ç‚¹
        if lm_list:
            print(f"æ£€æµ‹åˆ°æ‰‹éƒ¨! é£ŸæŒ‡æŒ‡å°–åæ ‡: {lm_list[8]}")

        # æ˜¾ç¤ºå›¾åƒ
        cv2.imshow("Hand Tracking", img)

        # æŒ‰ 'q' é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š
```bash
python my_first_gesture.py
```

## ç¬¬å››æ­¥ï¼šæ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹

```python
from wujihandpy import HandDetector

detector = HandDetector()
# ... è·å–å›¾åƒå ...
landmarks = detector.find_position(img)

# 21 ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹ï¼š
# 0: æ‰‹è…•
# 4: æ‹‡æŒ‡å°–
# 8: é£ŸæŒ‡å°–
# 12: ä¸­æŒ‡å°–
# 16: æ— åæŒ‡å°–
# 20: å°æŒ‡å°–
```

### 2. æ‰‹åŠ¿è¯†åˆ«

```python
from wujihandpy import GestureRecognizer

recognizer = GestureRecognizer()
gesture = recognizer.recognize_gesture(landmarks)

# æ”¯æŒçš„æ‰‹åŠ¿ï¼š
# - "open_palm": å¼ å¼€æ‰‹æŒ
# - "fist": æ¡æ‹³
# - "pointing": æŒ‡å‘
# - "pinch": æåˆï¼ˆæ‹‡æŒ‡+é£ŸæŒ‡ï¼‰
# - "victory": Vå­—æ‰‹åŠ¿
# - "thumbs_up": ç‚¹èµ
```

### 3. è™šæ‹Ÿé¼ æ ‡æ§åˆ¶

```python
from wujihandpy import VirtualMouse

mouse = VirtualMouse()

# ä½¿ç”¨é£ŸæŒ‡ä½ç½®ç§»åŠ¨é¼ æ ‡
if landmarks:
    index_finger_tip = landmarks[8]
    mouse.move(index_finger_tip[1], index_finger_tip[2])

# æ£€æµ‹æåˆæ‰‹åŠ¿è¿›è¡Œç‚¹å‡»
if recognizer.recognize_gesture(landmarks) == "pinch":
    mouse.click()
```

### 4. æ‰‹åŠ¿æ§åˆ¶è®¡ç®—æœº

åˆ›å»ºæ–‡ä»¶ `gesture_control.py`ï¼š

```python
from wujihandpy import HandDetector, GestureRecognizer, VirtualMouse
import cv2

def main():
    detector = HandDetector(max_hands=1)
    recognizer = GestureRecognizer()
    mouse = VirtualMouse()

    cap = cv2.VideoCapture(0)
    screen_width, screen_height = 1920, 1080  # è°ƒæ•´ä¸ºä½ çš„å±å¹•åˆ†è¾¨ç‡

    while True:
        success, img = cap.read()
        if not success:
            break

        img = detector.find_hands(img)
        lm_list = detector.find_position(img, draw=False)

        if lm_list:
            # è·å–é£ŸæŒ‡æŒ‡å°–ä½ç½®
            x, y = lm_list[8][1], lm_list[8][2]

            # è½¬æ¢åˆ°å±å¹•åæ ‡
            screen_x = int(x * screen_width / img.shape[1])
            screen_y = int(y * screen_height / img.shape[0])

            # ç§»åŠ¨é¼ æ ‡
            mouse.move(screen_x, screen_y)

            # æ£€æµ‹æ‰‹åŠ¿
            gesture = recognizer.recognize_gesture(lm_list)

            if gesture == "pinch":
                mouse.click()
                cv2.putText(img, "Click!", (50, 50),
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            elif gesture == "fist":
                # æ¡æ‹³å¯ä»¥æ‰§è¡Œå…¶ä»–æ“ä½œ
                pass

        cv2.imshow("Gesture Control", img)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

## ç¬¬äº”æ­¥ï¼šé«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æ‰‹åŠ¿è¯†åˆ«

```python
from wujihandpy import GestureDetector

class MyGestureDetector(GestureDetector):
    def detect_custom_gesture(self, landmarks):
        """æ£€æµ‹è‡ªå®šä¹‰æ‰‹åŠ¿"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯"OK"æ‰‹åŠ¿
        # æ‹‡æŒ‡å’Œé£ŸæŒ‡å½¢æˆåœ†åœˆï¼Œå…¶ä»–æ‰‹æŒ‡ä¼¸ç›´
        thumb_tip = landmarks[4]
        index_tip = landmarks[8]

        # è®¡ç®—æ‹‡æŒ‡å’Œé£ŸæŒ‡è·ç¦»
        distance = ((thumb_tip[1] - index_tip[1])**2 +
                   (thumb_tip[2] - index_tip[2])**2)**0.5

        if distance < 30:  # æåˆ
            # æ£€æŸ¥å…¶ä»–æ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´
            middle_up = landmarks[12][2] < landmarks[10][2]
            ring_up = landmarks[16][2] < landmarks[14][2]
            pinky_up = landmarks[20][2] < landmarks[18][2]

            if middle_up and ring_up and pinky_up:
                return "OK_sign"

        return None
```

### 2. å¤šæ‰‹åŠ¿ç»„åˆæ§åˆ¶

```python
class MultiGestureController:
    def __init__(self):
        self.gesture_history = []

    def process_gesture_sequence(self, gesture):
        """å¤„ç†æ‰‹åŠ¿åºåˆ—"""
        self.gesture_history.append(gesture)

        # ä¿ç•™æœ€è¿‘5ä¸ªæ‰‹åŠ¿
        if len(self.gesture_history) > 5:
            self.gesture_history.pop(0)

        # æ£€æµ‹ç‰¹å®šåºåˆ—ï¼šå¼ å¼€ -> æ¡æ‹³ -> å¼ å¼€
        sequence = [g['gesture'] for g in self.gesture_history]

        if sequence == ['open_palm', 'fist', 'open_palm']:
            return "zoom_in"  # æ‰§è¡Œæ”¾å¤§æ“ä½œ

        return None
```

### 3. å¹³æ»‘æ‰‹åŠ¿æ§åˆ¶

```python
from collections import deque

class SmoothController:
    def __init__(self, window_size=5):
        self.position_buffer = deque(maxlen=window_size)

    def smooth_position(self, x, y):
        """å¹³æ»‘ä½ç½®å˜åŒ–ï¼Œå‡å°‘æŠ–åŠ¨"""
        self.position_buffer.append((x, y))

        # è®¡ç®—å¹³å‡ä½ç½®
        avg_x = sum(p[0] for p in self.position_buffer) / len(self.position_buffer)
        avg_y = sum(p[1] for p in self.position_buffer) / len(self.position_buffer)

        return int(avg_x), int(avg_y)
```

### 4. æ·»åŠ å¯è§†åŒ–åé¦ˆ

```python
def draw_gesture_info(img, gesture, confidence):
    """åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºæ‰‹åŠ¿ä¿¡æ¯"""

    # ç»˜åˆ¶çŠ¶æ€æ¡†
    cv2.rectangle(img, (10, 10), (300, 100), (0, 255, 0), 2)

    # æ˜¾ç¤ºæ‰‹åŠ¿åç§°
    cv2.putText(img, f"Gesture: {gesture}",
               (20, 40), cv2.FONT_HERSHEY_SIMPLEX,
               1, (0, 255, 0), 2)

    # æ˜¾ç¤ºç½®ä¿¡åº¦
    cv2.putText(img, f"Confidence: {confidence:.2f}",
               (20, 80), cv2.FONT_HERSHEY_SIMPLEX,
               0.7, (0, 255, 0), 2)

    return img
```

## ç¬¬å…­æ­¥ï¼šå®æˆ˜é¡¹ç›®

### é¡¹ç›®1ï¼šæ‰‹åŠ¿æ¼”ç¤ºæ§åˆ¶å™¨

åˆ›å»º `presentation_controller.py`ï¼š

```python
from wujihandpy import HandDetector, GestureRecognizer
import cv2
import pyautogui

class PresentationController:
    def __init__(self):
        self.detector = HandDetector(max_hands=1)
        self.recognizer = GestureRecognizer()
        self.current_action = None

    def run(self):
        cap = cv2.VideoCapture(0)

        while True:
            success, img = cap.read()
            if not success:
                break

            img = self.detector.find_hands(img)
            lm_list = self.detector.find_position(img)

            if lm_list:
                gesture = self.recognizer.recognize_gesture(lm_list)

                # ä¸åŒæ‰‹åŠ¿æ‰§è¡Œä¸åŒæ“ä½œ
                if gesture == "pointing" and self.current_action != "next":
                    pyautogui.press('right')  # ä¸‹ä¸€é¡µ
                    self.current_action = "next"
                    print("ä¸‹ä¸€é¡µ")

                elif gesture == "victory" and self.current_action != "prev":
                    pyautogui.press('left')  # ä¸Šä¸€é¡µ
                    self.current_action = "prev"
                    print("ä¸Šä¸€é¡µ")

                elif gesture == "fist" and self.current_action != "exit":
                    # æŒ‰ä½ä¸æ”¾å¯ä»¥é€€å‡º
                    self.current_action = "exit"

                elif gesture == "open_palm":
                    self.current_action = None

                # æ˜¾ç¤ºå½“å‰æ‰‹åŠ¿
                cv2.putText(img, f"Gesture: {gesture}",
                          (10, 50), cv2.FONT_HERSHEY_SIMPLEX,
                          1, (0, 255, 0), 2)

            cv2.imshow("Presentation Control", img)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    controller = PresentationController()
    controller.run()
```

### é¡¹ç›®2ï¼šæ‰‹åŠ¿ç»˜å›¾æ¿

åˆ›å»º `gesture_draw.py`ï¼š

```python
from wujihandpy import HandDetector
import cv2
import numpy as np

class GestureDraw:
    def __init__(self):
        self.detector = HandDetector(max_hands=1)
        self.canvas = None
        self.prev_pos = None
        self.drawing = False
        self.color = (255, 0, 0)  # è“è‰²
        self.brush_size = 5

    def run(self):
        cap = cv2.VideoCapture(0)

        while True:
            success, img = cap.read()
            if not success:
                break

            # åˆå§‹åŒ–ç”»å¸ƒ
            if self.canvas is None:
                self.canvas = np.zeros_like(img)

            img = self.detector.find_hands(img)
            lm_list = self.detector.find_position(img)

            if lm_list:
                # é£ŸæŒ‡æŒ‡å°–ä½ç½®
                x, y = lm_list[8][1], lm_list[8][2]

                # æ£€æµ‹æ˜¯å¦æåˆï¼ˆç”»ç”»æ¨¡å¼ï¼‰
                thumb_tip = lm_list[4]
                index_tip = lm_list[8]
                distance = ((thumb_tip[1] - index_tip[1])**2 +
                           (thumb_tip[2] - index_tip[2])**2)**0.5

                if distance < 30:  # æåˆçŠ¶æ€
                    self.drawing = True
                    if self.prev_pos:
                        cv2.line(self.canvas, self.prev_pos, (x, y),
                                self.color, self.brush_size)
                    self.prev_pos = (x, y)
                else:
                    self.drawing = False
                    self.prev_pos = None

            # åˆå¹¶æ‘„åƒå¤´ç”»é¢å’Œç”»å¸ƒ
            img = cv2.addWeighted(img, 0.7, self.canvas, 0.3, 0)

            # æ˜¾ç¤ºæ¨¡å¼
            mode = "Drawing" if self.drawing else "Moving"
            cv2.putText(img, f"Mode: {mode}",
                      (10, 50), cv2.FONT_HERSHEY_SIMPLEX,
                      1, (0, 255, 0), 2)

            cv2.imshow("Gesture Drawing", img)

            # æŒ‰ 'c' æ¸…é™¤ç”»å¸ƒ
            if cv2.waitKey(1) & 0xFF == ord('c'):
                self.canvas = np.zeros_like(img)
            # æŒ‰ 'q' é€€å‡º
            elif cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    draw = GestureDraw()
    draw.run()
```

## ç¬¬ä¸ƒæ­¥ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æé«˜å¸§ç‡

```python
# é™ä½æ£€æµ‹é¢‘ç‡ï¼Œæ¯3å¸§æ£€æµ‹ä¸€æ¬¡
frame_count = 0
detection_every = 3

while True:
    success, img = cap.read()

    if frame_count % detection_every == 0:
        lm_list = detector.find_position(img)
        # ä½¿ç”¨è¿½è¸ªæŠ€æœ¯æ›´æ–°ä½ç½®
    else:
        # ç®€å•çš„å…‰æµè¿½è¸ª
        pass

    frame_count += 1
```

### 2. å¤šçº¿ç¨‹å¤„ç†

```python
import threading

class HandDetectorThread:
    def __init__(self):
        self.detector = HandDetector()
        self.latest_frame = None
        self.latest_result = None

    def process_frame(self, frame):
        while True:
            if self.latest_frame is not None:
                self.latest_result = self.detector.find_hands(self.latest_frame)

    def start(self, cap):
        thread = threading.Thread(target=self.process_frame, args=(cap,))
        thread.daemon = True
        thread.start()
```

### 3. GPU åŠ é€Ÿ

```python
# å¦‚æœæœ‰ NVIDIA GPUï¼Œå¯ä»¥å¯ç”¨ CUDA
detector = HandDetector(
    mode=False,
    max_hands=2,
    detection_con=0.7,
    tracking_con=0.5,
    model_complexity=1,  # 0, 1, 2 - è¶Šé«˜è¶Šç²¾ç¡®ä½†è¶Šæ…¢
    enable_gpu=True      # å¯ç”¨ GPU åŠ é€Ÿ
)
```

## å¸¸è§é—®é¢˜è§£å†³

### Q1: æ‘„åƒå¤´æ‰“ä¸å¼€
```bash
# æ£€æŸ¥æ‘„åƒå¤´è®¾å¤‡
python -c "import cv2; print(cv2.VideoCapture(0).isOpened())"

# å°è¯•ä¸åŒçš„æ‘„åƒå¤´ç´¢å¼•
cap = cv2.VideoCapture(0)  # æˆ– 1, 2, 3
```

### Q2: æ‰‹åŠ¿è¯†åˆ«ä¸å‡†ç¡®
```python
# è°ƒæ•´æ£€æµ‹å‚æ•°
detector = HandDetector(
    detection_con=0.5,  # é™ä½æ£€æµ‹é˜ˆå€¼
    tracking_con=0.7    # æé«˜è¿½è¸ªé˜ˆå€¼
)

# æ·»åŠ å…‰ç…§è¡¥å¿
img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
# ... å¤„ç†å…‰ç…§
```

### Q3: å»¶è¿Ÿå¤ªé«˜
```python
# é™ä½å›¾åƒåˆ†è¾¨ç‡
img = cv2.resize(img, (640, 480))

# å‡å°‘æ£€æµ‹çš„æ‰‹çš„æ•°é‡
detector = HandDetector(max_hands=1)
```

## ä¸‹ä¸€æ­¥å­¦ä¹ 

1. ğŸ“š æ·±å…¥å­¦ä¹  MediaPipe æ‰‹éƒ¨å…³é”®ç‚¹ç®—æ³•
2. ğŸ¯ å®ç°æ›´å¤æ‚çš„æ‰‹åŠ¿ç»„åˆ
3. ğŸ¨ å¼€å‘è‡ªå·±çš„æ‰‹åŠ¿åº”ç”¨
4. âš¡ ä¼˜åŒ–æ€§èƒ½å’Œå‡†ç¡®æ€§
5. ğŸŒ é›†æˆåˆ°å®é™…äº§å“ä¸­

## å‚è€ƒèµ„æº

- [Wujihandpy GitHub](https://github.com/wuji-technology/wujihandpy)
- [MediaPipe æ‰‹éƒ¨è¿½è¸ªæ–‡æ¡£](https://google.github.io/mediapipe/solutions/hands.html)
- [OpenCV å®˜æ–¹æ–‡æ¡£](https://docs.opencv.org/4.x/)

---

**æ­å–œï¼æ‚¨å·²ç»æŒæ¡äº† wujihandpy çš„åŸºæœ¬ç”¨æ³•ï¼** ğŸ‰

ç°åœ¨å¯ä»¥å¼€å§‹åˆ›å»ºè‡ªå·±çš„æ‰‹åŠ¿æ§åˆ¶åº”ç”¨äº†ï¼
