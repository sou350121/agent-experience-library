# RAG：Agent 的外部长期记忆库

**RAG (Retrieval-Augmented Generation)**，即检索增强生成。如果把 LLM 比作一个博学但记不住你私事、且知识有截止日期的“教授”，那么 RAG 就是给这位教授配了一套**实时更新的档案柜**。

## 1. 为什么 Agent 必须拥有 RAG？

Agent 的原生限制在于其“上下文窗口”是有限且昂贵的。RAG 解决了以下三个痛点：

1. **私有知识主权**：你的笔记、公司内部文档、未公开的代码，Agent 无法在训练数据中获得。
2. **知识实时性**：昨天的行业新闻或你刚刚修改的代码，大模型无法预知。
3. **成本效益**：微调 (Fine-tuning) 模型非常昂贵且难以更新，而 RAG 增删改查数据的成本极低。

## 2. RAG 的经典三部曲：摄取 ➡️ 检索 ➡️ 生成

### 第一步：数据摄取 (Indexing)
- **切片 (Chunking)**：将长文档切成小块（如每块 500 字）。
- **向量化 (Embedding)**：利用模型（如 OpenAI `text-embedding-3-small`）将文字转为一串数字（向量）。
- **存储**：存入向量数据库（如 `pgvector`, `Chroma`, `Pinecone`）。

### 第二步：智能检索 (Retrieval)
- 当你提问时，Agent 会将你的问题也转为向量。
- 在数据库中寻找“距离最近”（意思最接近）的几个数据块。

### 第三步：增强生成 (Generation)
- Agent 将找到的数据块和你的问题一起喂给大模型：
  > “请根据以下参考资料回答问题：[资料块 A] [资料块 B]... 问题：[你的问题]”

## 3. 从 Naive RAG 到 Agentic RAG

在高级 Agent 架构中，RAG 不再是被动触发，而是**主动决策**：

| 特性 | 传统 RAG (Naive) | Agentic RAG |
| :--- | :--- | :--- |
| **触发方式** | 用户问什么，就查什么 | Agent 判断是否需要查资料，查几次 |
| **检索策略** | 固定的语义搜索 | 会根据初步结果调整搜索词（Query Rewrite） |
| **结果评估** | 直接输出 | 会反思检索到的资料是否足以回答问题，不足则重新查 |

## 4. RAG 的选型建议 (配合“灯塔与火把”)

- **🔥 火把方案 (隐私至上)**：
  - 使用 **Ollama** 运行本地 Embedding 模型。
  - 使用 **PostgreSQL (pgvector)** 存储。
  - 适合：处理私人日记、公司机密代码、医疗记录。
- **🗼 灯塔方案 (性能优先)**：
  - 使用 **OpenAI API** 的向量模型。
  - 使用 **Pinecone** 或 **Milvus** 等云原生向量库。
  - 适合：大规模公共知识库、高性能搜索服务。

## 5. 核心认知：RAG 不是万能药
RAG 适合处理“事实性查询”，但不擅长处理“跨全文的逻辑总结”。如果你的需求是“总结这一万个文件里的核心思想”，单纯的 RAG 会顾此失彼，这时需要配合 **GraphRAG**（图搜索）或 **Long Context**（长上下文模型）。

---
> 💡 **总结**：RAG 赋予了 Agent “查阅档案”的能力，它是将通用 AI 转化为**垂直领域专家**的必经之路。

