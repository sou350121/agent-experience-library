# vLLM 是什么？为什么叫 vLLM？

## 1. vLLM 是什么

**vLLM** 是一个开源的 **LLM 推理/服务引擎**：
- 把你下载的模型权重（例如 Qwen/DeepSeek/GLM）**加载起来跑推理**
- 并以 **HTTP API** 的形式对外提供服务（常见是 OpenAI 兼容接口）

一句话：
- **模型（本地大模型）= 要跑的东西**
- **vLLM = 让它跑得更稳更快、并对外提供 API 的引擎**

## 2. 这和“API”有什么差别？

**没有差别：vLLM 本身就是在给你提供一个 API。**
差别在于“API 由谁托管”：
- 云 API：厂商托管，你调用对方地址
- vLLM：你自己托管（本机/内网），你调用你自己的地址

## 3. 为什么叫 vLLM？

vLLM 的命名通常被理解为 **Virtual Large Language Model**（虚拟大模型）。

它的“v/virtual”更像一个工程隐喻：
- 你面对的不是某个具体模型，而是一个 **可被统一调用的“模型服务层”**
- 在应用侧，你像在使用“一个 LLM”（统一 API），但底层可以是不同模型/不同并发调度/不同硬件

换句话说：
- **模型在你硬盘上是“实体”**（weights）
- **通过 vLLM 暴露出来的是“虚拟出来的服务”**（一个可调用、可并发、可替换的 LLM 入口）

## 4. 什么时候你会需要 vLLM

- 你要在本地/内网部署模型服务（成本/延迟/合规）
- 你要更好的并发吞吐与更稳定的服务化能力
- 你希望你的 Agent/应用用 HTTP 直接调用模型，而不是手动跑脚本

## 5. 进一步阅读

- vLLM 官网：`https://vllm.ai/`
- vLLM 语义路由（Semantic Routing）深度解析：`../capabilities/vllm-semantic-routing-deep-dive.mdx`
